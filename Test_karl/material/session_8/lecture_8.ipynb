{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 8\n",
    "## Scraping 1 - Data Collection\n",
    "*Snorre Ralund *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation\n",
    "<img src=\"https://i.imgflip.com/2fg9un.jpg\" title=\"made at imgflip.com\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting information and data on a subject of interest is no longer reduced to *tedious* survey designs, and *expensive* collections, *time consuming* interviews and *standardized* register data. \n",
    "\n",
    "This session will teach you how to harvest some of the billions of data points being generated and shared on the internet everyday.\n",
    "\n",
    "** We need data now! fast, free of charge, and in abundance.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "**Collecting raw data** (*Tomorrow we shall focus on parsing and cleaning*)\n",
    "\n",
    "* Lots of hand-on examples.\n",
    "* The basics of webscraping\n",
    "    * Connecting, Crawling, Parsing, Storing, Logging.\n",
    "* Ethical and Legal issues around scraping publicly available data.\n",
    "* Hacks: Backdoors, url construction and analyzing webpages.\n",
    "* Reliability of your data collection.\n",
    "\n",
    "**Main take-aways**\n",
    "- Utilize the data sources around you.\n",
    "- Knowing how to build your own custom datasets from web sources, without having to spend a month manually curating the data.\n",
    "- Get to know some of the most valuable tricks, \n",
    "- And instructions on how to Handle with care."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://i.imgflip.com/2fgdqb.jpg\" title=\"made at imgflip.com\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Ethics / Legal Issues\n",
    "* If a regular user can’t access it, we shouldn’t try to get it (That is considered hacking)https://www.dr.dk/nyheder/penge/gjorde-opmaerksom-paa-cpr-hul-nu-bliver-han-politianmeldt-hacking. \n",
    "* Don't hit it to fast: Essentially a DENIAL OF SERVICE attack (DOS). [Again considered hacking](https://www.dr.dk/nyheder/indland/folketingets-hjemmeside-ramt-af-hacker-angreb). \n",
    "* Add headers stating your name and email with your requests to ensure transparency. \n",
    "* Be careful with copyrighted material.\n",
    "* Fair use (don't take everything)\n",
    "* If monetizing on the data, be careful not to be in direct competition with whom you are taking the data from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"https://github.com/snorreralund/images/raw/master/Sk%C3%A6rmbillede%202017-08-03%2014.46.32.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages used\n",
    "Today we will mainly build on the python skills you have gotten so far, and tomorrow we will look into more specialized packages.\n",
    "\n",
    "* for connecting to the internet we use: **requests**\n",
    "* for parsing: **beautifulsoup** and **regex**\n",
    "* for automatic browsing / screen scraping (not covered in detail here): **selenium** \n",
    "* for behaving responsibly we use: **time** and ***our minds***\n",
    "\n",
    "We will write our scrapers with basic python, for larger projects consider looking into the packages **scrapy** or **pyspider**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, re, time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# quick 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scrape information from the web is:\n",
    "\n",
    "\t    1. Finding URLs of the pages containing the information you want.\n",
    "\t    2. Fetching the pages via HTTP.\n",
    "\t    3. Extracting the information from HTML.\n",
    "\t    4. Finding more URL containing what you want, go back to 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Connecting to the internet**\n",
    "**HTTP**\n",
    "\n",
    "*URL* : the adressline in our browser.\n",
    "\n",
    "Via HTTP we send a **get** request to an *address* with *instructions* ( - or rather our dns service provider redirects our request to the right address)\n",
    "\n",
    "*Address*: www.google.com\n",
    "\n",
    "*Instructions*: /trends?query=social+data+science\n",
    "\n",
    "*Header*: information send along with the request, including user agent (operating system, browser), cookies, and prefered encoding.\n",
    "\n",
    "*HTML*: HyperTextMarkupLanguage the language of displaying web content. More on this tomorrow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now** we are ready to get some data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3 basic examples**\n",
    "Lets get some action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** collecting data on display **\n",
    "** static webpage example **\n",
    "\n",
    "visit the following website (https://www.basketball-reference.com/leagues/NBA_2018.html).\n",
    "\n",
    "The page displays tables of data that we want to collect.\n",
    "Tomorrow you will see how to parse such a table, but for now I want to show you a neat function that has already implemented this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Eastern Conference</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>W/L%</th>\n",
       "      <th>GB</th>\n",
       "      <th>PS/G</th>\n",
       "      <th>PA/G</th>\n",
       "      <th>SRS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toronto Raptors* (1)</td>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>0.720</td>\n",
       "      <td>—</td>\n",
       "      <td>111.7</td>\n",
       "      <td>103.9</td>\n",
       "      <td>7.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Boston Celtics* (2)</td>\n",
       "      <td>55</td>\n",
       "      <td>27</td>\n",
       "      <td>0.671</td>\n",
       "      <td>4.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>100.4</td>\n",
       "      <td>3.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Philadelphia 76ers* (3)</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>0.634</td>\n",
       "      <td>7.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>105.3</td>\n",
       "      <td>4.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cleveland Cavaliers* (4)</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>0.610</td>\n",
       "      <td>9.0</td>\n",
       "      <td>110.9</td>\n",
       "      <td>109.9</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Indiana Pacers* (5)</td>\n",
       "      <td>48</td>\n",
       "      <td>34</td>\n",
       "      <td>0.585</td>\n",
       "      <td>11.0</td>\n",
       "      <td>105.6</td>\n",
       "      <td>104.2</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Miami Heat* (6)</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>0.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>102.9</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Milwaukee Bucks* (7)</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>0.537</td>\n",
       "      <td>15.0</td>\n",
       "      <td>106.5</td>\n",
       "      <td>106.8</td>\n",
       "      <td>-0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Washington Wizards* (8)</td>\n",
       "      <td>43</td>\n",
       "      <td>39</td>\n",
       "      <td>0.524</td>\n",
       "      <td>16.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>106.0</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Detroit Pistons (9)</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>0.476</td>\n",
       "      <td>20.0</td>\n",
       "      <td>103.8</td>\n",
       "      <td>103.9</td>\n",
       "      <td>-0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Charlotte Hornets (10)</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>0.439</td>\n",
       "      <td>23.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>New York Knicks (11)</td>\n",
       "      <td>29</td>\n",
       "      <td>53</td>\n",
       "      <td>0.354</td>\n",
       "      <td>30.0</td>\n",
       "      <td>104.5</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Brooklyn Nets (12)</td>\n",
       "      <td>28</td>\n",
       "      <td>54</td>\n",
       "      <td>0.341</td>\n",
       "      <td>31.0</td>\n",
       "      <td>106.6</td>\n",
       "      <td>110.3</td>\n",
       "      <td>-3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Chicago Bulls (13)</td>\n",
       "      <td>27</td>\n",
       "      <td>55</td>\n",
       "      <td>0.329</td>\n",
       "      <td>32.0</td>\n",
       "      <td>102.9</td>\n",
       "      <td>110.0</td>\n",
       "      <td>-6.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Orlando Magic (14)</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>0.305</td>\n",
       "      <td>34.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.2</td>\n",
       "      <td>-4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Atlanta Hawks (15)</td>\n",
       "      <td>24</td>\n",
       "      <td>58</td>\n",
       "      <td>0.293</td>\n",
       "      <td>35.0</td>\n",
       "      <td>103.4</td>\n",
       "      <td>108.8</td>\n",
       "      <td>-5.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Eastern Conference   W   L   W/L%    GB   PS/G   PA/G   SRS\n",
       "0       Toronto Raptors* (1)  59  23  0.720     —  111.7  103.9  7.29\n",
       "1        Boston Celtics* (2)  55  27  0.671   4.0  104.0  100.4  3.23\n",
       "2    Philadelphia 76ers* (3)  52  30  0.634   7.0  109.8  105.3  4.30\n",
       "3   Cleveland Cavaliers* (4)  50  32  0.610   9.0  110.9  109.9  0.59\n",
       "4        Indiana Pacers* (5)  48  34  0.585  11.0  105.6  104.2  1.18\n",
       "5            Miami Heat* (6)  44  38  0.537  15.0  103.4  102.9  0.15\n",
       "6       Milwaukee Bucks* (7)  44  38  0.537  15.0  106.5  106.8 -0.45\n",
       "7    Washington Wizards* (8)  43  39  0.524  16.0  106.6  106.0  0.53\n",
       "8        Detroit Pistons (9)  39  43  0.476  20.0  103.8  103.9 -0.26\n",
       "9     Charlotte Hornets (10)  36  46  0.439  23.0  108.2  108.0  0.07\n",
       "10      New York Knicks (11)  29  53  0.354  30.0  104.5  108.0 -3.53\n",
       "11        Brooklyn Nets (12)  28  54  0.341  31.0  106.6  110.3 -3.67\n",
       "12        Chicago Bulls (13)  27  55  0.329  32.0  102.9  110.0 -6.84\n",
       "13        Orlando Magic (14)  25  57  0.305  34.0  103.4  108.2 -4.92\n",
       "14        Atlanta Hawks (15)  24  58  0.293  35.0  103.4  108.8 -5.30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.basketball-reference.com/leagues/NBA_2018.html' # link to the website\n",
    "import pandas as pd \n",
    "dfs = pd.read_html(url) # parses all tables found on the page.\n",
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_df = pd.read_html(url,attrs={'id':'confs_standings_E'}) # only parse the tables with attribute confs_standings_E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** collecting data behind the display**\n",
    "** dynamic webpage example **\n",
    "\n",
    "Websites that continually show new data (jobsites, real-estate pages, social media), are as a rule dynamic webpages, where the whole page is not send as raw HTML. Instead a set of instructions (JavaScripts) on how to build it is send. Within those instructions we can often find direct calls to the data displayed.\n",
    "\n",
    "Click on the following link: https://trends.google.com/trends/explore?date=all&geo=DK&q=H%C3%A5ndbold,Fodbold\n",
    "\n",
    "Here we want to collect the data behind the graph. We open your browsers **>Network Monitor<** tool and search for the request that contains the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url = 'https://trends.google.com/trends/api/widgetdata/multiline?hl=da&tz=-120&req={\"time\":\"2004-01-01+2018-08-07\",\"resolution\":\"MONTH\",\"locale\":\"da\",\"comparisonItem\":[{\"geo\":{\"country\":\"DK\"},\"complexKeywordsRestriction\":{\"keyword\":[{\"type\":\"BROAD\",\"value\":\"Håndbold\"}]}},{\"geo\":{\"country\":\"DK\"},\"complexKeywordsRestriction\":{\"keyword\":[{\"type\":\"BROAD\",\"value\":\"Fodbold\"}]}}],\"requestOptions\":{\"property\":\"\",\"backend\":\"IZG\",\"category\":0}}&token=APP6_UEAAAAAW2s2Bpioky7lfJMWt4LDyhLInpC0jFzC&tz=-120'\n",
    "response = requests.get(url)\n",
    "response.ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "now we unpack and plot the data \n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-b1afc785f5b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'd' is not defined"
     ]
    }
   ],
   "source": [
    "d.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-64e02aa47748>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ggplot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'default'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timelineData'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json # we use the json module to parse the json string.\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime # datetime module is used to handle time information in python\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "data = json.loads(response.text.split(',',1)[1].strip())\n",
    "t,y,y1 = zip(*[(i['time'],i['value'][0],i['value'][1]) for i in data['default']['timelineData']])\n",
    "t = [datetime.datetime.fromtimestamp(int(i)) for i in t]\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(t,y,label='Handball')\n",
    "plt.plot(t,y1,label='Football')\n",
    "plt.legend()\n",
    "plt.title('Danish Search Activity')\n",
    "plt.ylabel('google trends')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In-class exercise 1** \n",
    "* Click on the following link (https://coinmarketcap.com/currencies/bitcoin/#charts)\n",
    "* open the **>Network Monitor<** of your browser (refresh the page) and figure which request is collecting the data behind the chart. \n",
    "* Collect the data using requests.\n",
    "* Plot the \"price_usd\" data against time. Data comes as a nested dictionary. \n",
    "    * First you need to unpack this.\n",
    "    * Each datapoint is a [unix timestamp](https://en.wikipedia.org/wiki/Unix_time) in miliseconds and a price. \n",
    "    * To plot against time in python, you need to convert it to a datetime object using the ``datetime`` module. \n",
    "\n",
    "``t = datetime.datetime.fromtimestamp(unixtime_in_seconds)``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-ca002498f6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_usd'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromtimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#print(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         raise TypeError(\"cannot convert the series to \"\n\u001b[0;32m--> 117\u001b[0;31m                         \"{0}\".format(str(converter)))\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "# solution goes here\n",
    "import datetime\n",
    "\n",
    "url = 'https://graphs2.coinmarketcap.com/currencies/bitcoin/'\n",
    "response = requests.get(url)\n",
    "response.ok\n",
    "dict = response.json()\n",
    "df = pd.DataFrame(dict['price_usd'])\n",
    "df[0] = str(df[0])\n",
    "df[0] = datetime.datetime.fromtimestamp(df[0])\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ** collecting unstructured data **\n",
    "Step 1. Finding the pages to collect.\n",
    "Often times we need to crawl a set of pages, from a website, this means finding all the links we need to collect.\n",
    "Here is a simple example of doing that.\n",
    "\n",
    "Say we wanted to investigate the difference in how many hours of lectures and exercises the students on different universities and different study programs gets, or even if the ressources to this area are dropping. To answer this question we decide to scrape the Course description and the university webpages. \n",
    "\n",
    "> First we look at the courses on UCPH: \n",
    "* Click on this link: https://kurser.ku.dk\n",
    "* Navigate to a page where links to courses are displayed. \n",
    "* Figure out a way to fetch those links.\n",
    "Here we look for the **\"a\"**-tag and the **\"href\"** attribute. \n",
    "\n",
    "Use your browsers **>Inspector<** to see the raw html that we want to parse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I have found a list of courses at Anthropology UCPH\n",
    "url = 'https://kurser.ku.dk/archive/2016-2017/STUDYBOARD_0010'\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------\n",
    "After inspecting the html using our browsers Inspector tool, we can see that links occur after a href= pattern. \n",
    "Employing the python you already know we can use the ``string.split`` method to fetch the links.\n",
    "______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/archive/2016-2017/course/AANB11002U', 'https://jobportal.ku.dk/']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we split by the pattern 'href=\"' and\n",
    "# skip the first element that was before the first occurrece of href\n",
    "link_locations = response.text.split('href=\"')[1:] \n",
    "###\n",
    "\n",
    "links = [] #define container for the links\n",
    "import random # good practice is to shuffle our data to inspect different data points each time.\n",
    "# we do this with the random.sample function.\n",
    "for link in random.sample(link_locations,len(link_locations)):\n",
    "    #print(link)\n",
    "    link = link.split('\"')[0]\n",
    "    links.append(link)\n",
    "links[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "# links are relative to the domain: https://kurser.ku.dk/\n",
    "links = ['https://kurser.ku.dk'+ i for i in links]\n",
    "print(len(links))\n",
    "# only links with /archive/ in the name is a relevant course\n",
    "links = [link for link in links if '/archive/' in link]\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://kurser.ku.dk/archive/2016-2017/course/AANB11075U', 'https://kurser.ku.dk/archive/2016-2017/course/AANK16102U', 'https://kurser.ku.dk/archive/2016-2017/course/AANB05070U', 'https://kurser.ku.dk/archive/2016-2017/course/AANB05023U', 'https://kurser.ku.dk/archive/2016-2017/course/AANB11046U']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(random.sample(links,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In-class exercise 2** Now it your turn to practice collecting links using the simple split method.\n",
    "\n",
    "The above example only collected links to courses in Anthropology. Now I want you to build a script that\n",
    "* first collects links to all the different studyboards here: https://kurser.ku.dk/archive/2016-2017\n",
    "\n",
    "* And next run through those to collect the links to all the courses at UCPH 2016-2017.\n",
    "\n",
    "* figure out how to get links from the other years (hint look at the urls).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2016-2017/STUDYBOARD_0010', '2016-2017/STUDYBOARD_0032', '2016-2017/STUDYBOARD_0018', '2016-2017/STUDYBOARD_0071', '2016-2017/STUDYBOARD_0021', '2016-2017/STUDYBOARD_0044', '2016-2017/STUDYBOARD_0009', '2016-2017/STUDYBOARD_PHD_0015', '2016-2017/STUDYBOARD_PB_01', '2016-2017/STUDYBOARD_0012', '2016-2017/STUDYBOARD_0033', '2016-2017/STUDYBOARD_0025', '2016-2017/STUDYBOARD_0026', '2016-2017/STUDYBOARD_MA_0002', '2016-2017/STUDYBOARD_0019', '2016-2017/STUDYBOARD_0113', '2016-2017/STUDYBOARD_0054', '2016-2017/STUDYBOARD_0028', '2016-2017/STUDYBOARD_0066', '2016-2017/STUDYBOARD_0027', '2016-2017/STUDYBOARD_0029', '2016-2017/STUDYBOARD_0034', '2016-2017/STUDYBOARD_MA_0013', '2016-2017/STUDYBOARD_0030', '2016-2017/STUDYBOARD_0031', '2016-2017/STUDYBOARD_PHD_0010', '2016-2017/STUDYBOARD_0020', '2016-2017/STUDYBOARD_0001', '2016-2017/STUDYBOARD_0005', '2016-2017/SUND_FG', '2016-2017/STUD_HBI', '2016-2017/STUDYBOARD_MA_0014', '2016-2017/STUDYBOARD_0006', '2016-2017/SUND_MAS', '2016-2017/STUDYBOARD_0016', '2016-2017/SUND_MS', '2016-2017/STUDYBOARD_0091', '2016-2017/STUDYBOARD_4861', '2016-2017/STUDYBOARD_0003', '2016-2017/STUDYBOARD_0203', '2016-2017/STUDYBOARD_0014', '2016-2017/STUDYBOARD_0004', '2016-2017/STUDYBOARD_0023']\n"
     ]
    }
   ],
   "source": [
    "# solution goes here\n",
    "url = 'https://kurser.ku.dk/archive/2016-2017'\n",
    "response = requests.get(url)\n",
    "#print(response.ok)\n",
    "link_locations = response.text.split('href=\"')[1:]\n",
    "#link_locations\n",
    "links = []\n",
    "\n",
    "for link in link_locations:\n",
    "    link = link.split('\"')[0]\n",
    "    links.append(link)\n",
    " \n",
    "links = [link for link in links if '2016-2017' in link]\n",
    "print(links)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating websites to collect links\n",
    "Now I will show you a few common ways of finding the links to the pages you want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Building URLS using a recognizable pattern.\n",
    "A nice trick is to understand how urls are constructed to communicate with a server. \n",
    "\n",
    "Lets look at how [jobindex.dk](https://www.jobindex.dk/) does it. We simply click around and take note at how the addressline changes.\n",
    "\n",
    "This will allow us to navigate the page, without having to parse information from the html or click any buttons.\n",
    "\n",
    "* / is like folders on your computer.\n",
    "* ? entails the start of a query with parameters \n",
    "* = defines a variable: e.g. page=1000 or offset = 100 or showNumber=20\n",
    "* & separates different parameters.\n",
    "* \\+ is html for whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Good practices\n",
    "* Transparency: send your email and name in the header so webmasters will know you are not a malicious actor.\n",
    "* Ratelimiting: Make sure you don't hit their servers to hard.\n",
    "* Reliability: \n",
    "    * Make sure the scraper can handle exceptions (e.g. bad connection) without crashing.\n",
    "    * Keep a log.\n",
    "    * Store raw data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'User-Agent': 'python-requests/2.18.4', 'Accept-Encoding': 'gzip, deflate', 'Accept': '*/*', 'Connection': 'keep-alive', 'email': 'youremail', 'name': 'name'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transparent scraping\n",
    "import requests\n",
    "#response = requests.get('https://www.google.com')\n",
    "session = requests.session()\n",
    "session.headers['email'] = 'youremail' \n",
    "session.headers['name'] = 'name'\n",
    "#session.headers['User-Agent'] = '' # sometimes you need to pose as another agent...\n",
    "session.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A quick tip is that you can change the user agent to a cellphone to obtain more simple formatting of the html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Reliability!\n",
    "When using found data, you are the curator and you are **responsible** for enscribing **trust** in the datacompilation.\n",
    "\n",
    "Reliability is ensured by an interative process, of inspection, error detection and error handling.\n",
    "\n",
    "Build your scrape around making this process easy by:\n",
    "* logging information about the collection (e.g. servertime, size of response to plot weird behavior, size of response over time,  number of calls pr day, detection of holes in your data).\n",
    "* Storing raw data (before parsing it) to be able to backtrack problems, without having to wait for the error to come up.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Control the pace of your calls\n",
    "import time\n",
    "def ratelimit():\n",
    "    \"A function that handles the rate of your calls.\"\n",
    "    time.sleep(1) # sleep one second.\n",
    "# Reliable requests\n",
    "def get(url,iterations=10,check_function=lambda x: x.ok):\n",
    "    \"\"\"This module ensures that your script does not crash from connection errors.\n",
    "        that you limit the rate of your calls\n",
    "        that you have some reliability check\n",
    "        iterations : Define number of iterations before giving up. \n",
    "        exceptions: Define which exceptions you accept, default is all. \n",
    "    \"\"\"\n",
    "    for iteration in range(iterations):\n",
    "        try:\n",
    "            # add ratelimit function call here\n",
    "            ratelimit() # !!\n",
    "            response = session.get(url)\n",
    "            if check_function(response):\n",
    "                return response # if succesful it will end the iterations here\n",
    "        except exceptions as e: #  find exceptions in the request library requests.exceptions\n",
    "            print(e) # print or log the exception message.\n",
    "    return None # your code will purposely crash if you don't create a check function later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Example of a logging function\n",
    "``` python \n",
    "done = set()# define a container for the links you have already collected\n",
    "count = 0\n",
    "def log_function(url,response,error_check=lambda x: x.ok,separator=','):\n",
    "    global last_t\n",
    "    if os.path.isfile(logfilepath)\n",
    "        f_log = open(logfilepath,'w') # define logfile, remember not to overwrite it.\n",
    "        # write columns to be used, basic ones are, servertime, deltaT since last call, url, success of request, \n",
    "        header = ['serverTime','deltaT','url','success','length','path']\n",
    "        f_log.write(','.join(header)+'\\n')\n",
    "    else:\n",
    "        f_log = open(logfilepath,'a')\n",
    "    #### Update timing info ####\n",
    "    t = time.time()\n",
    "    delta_t = t-last_t # calculate time since last call\n",
    "    last_t = t# update last call time\n",
    "    #### meta data ### \n",
    "    success = error_check(response)\n",
    "    if success: # if call is successfull we add it to the done container\n",
    "        done.add(url)\n",
    "    if response.ok:\n",
    "        length = len(response.text)\n",
    "    else:\n",
    "        length = 0\n",
    "    row = [t,delta_t,url,success,length,path]\n",
    "    f_log.write(separator.join(map(str,row))+'\\n')\n",
    "``` \n",
    "\n",
    "\n",
    "``` python \n",
    "base_path = 'path_to_dir/%d'\n",
    "for link in links:\n",
    "    if link in done: #check if you have already downloaded the link\n",
    "        continue\n",
    "    count +=1\n",
    "    response = requests.get(link)\n",
    "    #define path \n",
    "    path = base_path%count # use the link count as a filename\n",
    "    if response.ok:\n",
    "        html = response.text\n",
    "        f.write(path,response+'\\n\\r')\n",
    "    # run your log function\n",
    "    log_function()\n",
    "``` \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
