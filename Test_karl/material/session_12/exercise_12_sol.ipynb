{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 12: Linear regression models.\n",
    "\n",
    "*Afternoon, August 20, 2018*\n",
    "\n",
    "In this Exercise Set 12 we will work with regression models.\n",
    "\n",
    "We import our standard stuff. Notice that we are not interested in seeing the convergence warning in scikit-learn so we suppress them for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.1: Implementing OLS solved with gradiant decent\n",
    " \n",
    "In this exercise we will try to implement the OLS-estimator from scratch, and solve using numerical optimation using the gradiant decent algorith. Then we will fit it to some data, and compare our own solution to the standard solution from `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.1**: Import the dataset `tips` from the `seaborn`.\n",
    "\n",
    "\n",
    "*Hint*: use the `load_dataset` method in seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the example tips dataset\n",
    "\n",
    "tips = sns.load_dataset(\"tips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.2**: Convert non-numeric variables to dummy variables for each category (remember to leave one column out for each catagorical variable, so you have a reference). Restructure the data so we get a dataset `y` containing the variable tip, and a dataset `X` containing the \n",
    "features. \n",
    "\n",
    ">> *Hint*: You might want to use the `get_dummies` method in pandas, with the `drop_first = True` parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips_num = pd.get_dummies(tips, drop_first=True)\n",
    "\n",
    "X = tips_num.drop('tip', axis = 1)\n",
    "y = tips_num['tip']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.3**: Divide the features and target into test and train data. Make the split 50 pct. of each. The split data should be called `X_train`, `X_test`, `y_train`, `y_test`.\n",
    "\n",
    ">> *Hint*: You may use `train_test_split` in `sklearn.model_selection`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.4**: Normalize your features by converting to zero mean and one std. deviation.\n",
    "\n",
    ">> *Hint 1*: Take a look at `StandardScaler` in `sklearn.preprocessing`. \n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "norm_scaler = StandardScaler().fit(X_train) \n",
    "X_train = norm_scaler.transform(X_train) \n",
    "X_test = norm_scaler.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.5**: Make a function called `compute_error` to compute the prediction errors given input target `y_`, input features `X_` and input weights `w_`. You should use matrix multiplication.\n",
    ">\n",
    ">> *Hint 1:* You can use the net-input fct. from yesterday.\n",
    ">>\n",
    ">> *Hint 2:* If you run the following code,\n",
    ">> ```python\n",
    "y__ = np.array([1,1])\n",
    "X__ = np.array([[1,0],[0,1]])\n",
    "w__ = np.array([0,1,1])\n",
    "compute_error(y__, X__, w__)\n",
    "```\n",
    "\n",
    ">> then you should get output:\n",
    "```python \n",
    "array([0,0])\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_input(X_, w_):    \n",
    "    ''' Computes the matrix product between X and w. Note that\n",
    "    X is assumed note to contain a bias/intercept column.'''\n",
    "    return np.dot(X_, w_[1:]) + w_[0]   # We have to add w_[0] separately because this is the constant term. We could also have added a constant term (columns of 1's to X_ and multipliced it to all of w_)\n",
    "\n",
    "def compute_error(y_, X_, w_):\n",
    "    return y_ - net_input(X_, w_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.6**: Make a function to update the weights given input target `y_`, input features `X_` and input weights `w_` as well as learning rate, $\\eta$, i.e. greek `eta`. You should use matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weight(y_, X_, w_, eta):\n",
    "    error = compute_error(y_, X_, w_)    \n",
    "    w_[1:] += eta * (X_.T.dot(error))\n",
    "    w_[0] += eta * (error).sum()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.7**: Use the code below to initialize weights `w` at zero given feature set `X`. Notice how we include an extra weight that includes the bias term. Set the learning rate `eta` to 0.001. Make a loop with 50 iterations where you iteratively apply your weight updating function. \n",
    "\n",
    ">```python\n",
    "w = np.zeros(1+X.shape[1])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.zeros(1+X.shape[1])\n",
    "error_train, error_test = [], []\n",
    "for i in range(50):\n",
    "    update_weight(y_train, X_train, w, 10**-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.8**: Make a function to compute the mean squared error. Alter the loop so it makes 100 iterations and computes the MSE for test and train after each iteration, plot these in one figure. \n",
    "\n",
    ">> Hint: You can use the following code to check that your model works:\n",
    ">>```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "assert((w[1:] - reg.coef_).sum() < 0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.1.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_, X_, w_):\n",
    "    error_squared = compute_error(y_, X_, w_)**2\n",
    "    return error_squared.sum() / len(y_)\n",
    "\n",
    "w = np.zeros(X.shape[1]+1)\n",
    "\n",
    "MSE_train = [MSE(y_train, X_train, w)]\n",
    "MSE_test = [MSE(y_test, X_test, w)]\n",
    "\n",
    "for i in range(100):\n",
    "    update_weight(y_train, X_train, w, 10**-3)\n",
    "    \n",
    "    MSE_train.append(MSE(y_train, X_train, w))\n",
    "    MSE_test.append(MSE(y_test, X_test, w))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x17316fe5e48>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17316feeac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(MSE_train).plot()\n",
    "pd.Series(MSE_test).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.9 (BONUS)**: Implement your linear regression model as a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ANSWER: A solution is found on p. 320 in Python for Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.1.10 (BONUS)**: Is it possible to adjust our linear model to become a Lasso? Is there a simple fix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ANSWER: No, we cannot exactly solve for the Lasso with gradient descent. However, we can make an approximate solution which is pretty close and quite intuitive - see good explanation [here](https://stats.stackexchange.com/questions/177800/why-proximal-gradient-descent-instead-of-plain-subgradient-methods-for-lasso)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 12.2: Houseprices\n",
    "In this example we will try to predict houseprices using a lot of variable (or features as they are called in Machine Learning). We are going to work with Kaggle's dataset on house prices, see information [here](https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Kaggle is an organization that hosts competitions in building predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 12.2.0:** Load the california housing data with scikit-learn using the code below. Inspect the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup\n",
      "10089  4.0893      35.0  5.267760   0.983607      1056.0  2.885246\n",
      "2136   3.7578      24.0  5.061538   0.957692       781.0  3.003846\n",
      "17546  2.4306      39.0  4.899209   1.069170      1990.0  3.932806\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cal_house = fetch_california_housing()    \n",
    "X = pd.DataFrame(data=cal_house['data'], \n",
    "                 columns=cal_house['feature_names'])\\\n",
    "             .iloc[:,:-2]\n",
    "y = cal_house['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=1)\n",
    "\n",
    "print(X_train.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "> **Ex.12.2.1**: Generate interactions between all features to third degree, make sure you **exclude** the bias/intercept term. How many variables are there? Will OLS fail? \n",
    "\n",
    "> After making interactions rescale the features to have zero mean, unit std. deviation. Should you use the distribution of the training data to rescale the test data?  \n",
    "\n",
    ">> *Hint 1*: Try importing `PolynomialFeatures` from `sklearn.preprocessing`\n",
    "\n",
    ">> *Hint 2*: If in doubt about which distribution to scale, you may read [this post](https://stats.stackexchange.com/questions/174823/how-to-apply-standardization-normalization-to-train-and-testset-if-prediction-i)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.1]\n",
    "\n",
    "# This will be in assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.2**: Estimate the Lasso model on the train data set, using values of $\\lambda$ in the range from $10^{-4}$ to $10^4$. For each $\\lambda$  calculate and save the Root Mean Squared Error (RMSE) for the test and train data. \n",
    "\n",
    "> *Hint*: use `logspace` in numpy to create the range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.2]\n",
    "\n",
    "# This will be in assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.12.2.3**: Make a plot with on the x-axis and the RMSE measures on the y-axis. What happens to RMSE for train and test data as $\\lambda$ increases? The x-axis should be log scaled. Which one are we interested in minimizing? \n",
    "\n",
    "> Bonus: Can you find the lambda that gives the lowest MSE-test score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 12.2.3]\n",
    "\n",
    "# This will be in assignment 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
