{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Session 15\n",
    "## Text as Data 1 - Supervised Learning and Text Classification\n",
    "*Snorre Ralund *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "![](http://3.bp.blogspot.com/-H5EfLEMD7gs/VU0R6T0cR5I/AAAAAAAABeA/3tTzaqPYdRw/s1600/desmazieres-borges-library-of-babel.jpg)\n",
    "\n",
    "Information about what people do, think, and feel lies embedded in text. \n",
    "\n",
    "Much of our social activity and communication is now done natively via text. \n",
    "\n",
    "The internet is full of information stored in text, and textual traces range from social media, emails and instant messaging, to automatically transcribed videos from youtube or memos from doctors, goverment records and digitized libraries. \n",
    "\n",
    "*\"This vast supply of text has broad demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data.\n",
    "And it has broad exciting opportunities for social scientists who knows how to get and handle text as data.\"* - **James Evans, University of Chicago, Director of Masters Programme in Computational Social Science**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Projects(1)\n",
    "#### Using supervised machine learning or dictionary methods to measure Phenomena in text\n",
    "Projects:\n",
    "- Gender Bias in Newspapers? \n",
    "    * What about Gender and Emotions in newspapers? \n",
    "    * Length of Reviews and gender?\n",
    "\n",
    "Own Projects:\n",
    "- Studying ethnic and political polarization. \n",
    "    * Measuring relational insult types and the saliency of these accross ethnicities in Denmark accross time using supervised learning on large scale social media data. \n",
    "- Analyzing developments in the Jobmarket. \n",
    "    * Datadriven, and using a dictionary approach, developed using a semiautomated search algorithm to generate it.\n",
    "- Conflict discourse and politization in Social Movements. Supervised learning.\n",
    "- Using it as a way of validating a proxy of friendship. Textual features to qualify what a tag on facebook means.\n",
    "- Analyzing growing Populism in Danish Politics. Measuring topics using a validated dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projects(2)\n",
    "#### Using techniques from information extraction, data mining and Natural Langauge Processing to discover, characterize and extract textual features used in a \n",
    "Projects:\n",
    "    * Predicting bitcoin prices... Extracting textual features and using those as input to a predictor.\n",
    "       - pretty hardÂ´task. \n",
    "       - however take a look at this paper: https://hobbs.human.cornell.edu/SensorsICWSM17.pdf\n",
    "       \n",
    "    * Growing Sensationalism, and dumbing down the news? Measuring complexity of the text (LIX-number) and length of title as click bate indicator.\n",
    "        * What about Emotions in news. How has it changed on DR? \n",
    "        * Do they cite more sources within the text (or do they just advertise their own content)?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "** Research Examples **\n",
    "* Chen 2013: *\"The effects of language on Economic Behaviour - Evidence from Savings Rates, Health Behaviors, and Retirement Assets\"*\n",
    "    * Quantifying the use of future tense in different languages and its connection to future oriented behavior: saving for retirement, practicing safe sex etc.\n",
    "    \n",
    "* Gentzkow and Shapiro 2007: *\"What drives media slant? Evidence from U.S daily newspapers\"*\n",
    "    * Measuring slant as document distances to the words of different political actors.\n",
    "    \n",
    "*  Danescu-Niculescu-M et. al 2012 [*\"Echoes of Power: Language Effects and Power Differences in Social Interaction\"*](https://www.cs.cornell.edu/%7Ecristian/Echoes_of_power_files/echoes_of_power.pdf)\n",
    "    * Measuring power dynamics as imitation of linguistic style. Where style is the use of word classes: *\"articles,  auxiliary  verbs,  conjunctions,  high-frequency  adverbs, impersonal pronouns, personal pronouns, prepositions, and quantifiers\"*, that can be automatically extracted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Agenda\n",
    "\n",
    "* Applying Supervised learning to text classification.\n",
    "* From Text to Vector. \n",
    "    * Bag of Words.\n",
    "    * Term frequency Inverse Document Frequency\n",
    "    * Ngrams\n",
    "    * Tokenization \n",
    "    * Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jbv933\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Packages\n",
    "import nltk # general framework\n",
    "#nltk.download('all') # Uncomment this and run\n",
    "import re # Regular expressions\n",
    "import gensim # For unsupervised learning and text \n",
    "# import spacy # A lot of Natural language processing capabilities (NER, POS, Sentiment analysis)\n",
    "# stanford core nlp written in java but you can also use it in python.\n",
    "## MORE import polyglot for multilanguage "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Getting from Text to vector\n",
    "Our supervised models do not know how to use the .split function or what to search for using regular expressions.\n",
    "\n",
    "![](https://image.slidesharecdn.com/bcw-cochrane-tech-2013-130927075508-phpapp02/95/text-mining-machine-learning-nlp-and-all-that-in-10-minutes-10-638.jpg?cb=1380268595)\n",
    "<center>Credit Byron C Wallace</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Vector representation \n",
    "## Bag of Words (BoW) or Document Term Matrix\n",
    "* Throw out the word order.\n",
    "* And let each word be a feature. --> map word to an index in a matrix.\n",
    "\n",
    "\n",
    "doc1: \"i really love bacon\"\n",
    "\n",
    "doc2: \"i really don't like bacon\"\n",
    "\n",
    "<center> **As a Document Term Matrix** <center>\n",
    "\n",
    "document | really | i | love | bacon | don't\n",
    "--- | --- | --- | --- | --- | --- | ---\n",
    "*doc1* | 0 | 0 | 1 | 1 | 0 |\n",
    "*doc2* | 1 | 1 | 0 | 1 | 1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of Words(2) - Problem with polesemy\n",
    "\n",
    "Consider the following to documents: \n",
    "\n",
    "doc1: *\"River A/S declared default by the bank.\"*\n",
    "\n",
    "doc2: *\"When camping my default is by the river bank.\"*\n",
    "\n",
    "<center> **As a Document Term Matrix** <center>\n",
    "\n",
    "document | declared | by | default | bank | river | a/s | when | camping | my\n",
    "--- | --- | --- | --- | --- | --- | --- | --- | --- | ---\n",
    "*doc1* | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 \n",
    "*doc2* | 0 | 1 | 1 | 1 | 1 | 0 | 1 | 1 | 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bag of Words (3) - Lack of word order\n",
    "doc1 = 'this was not the best movie'\n",
    "\n",
    "doc2 = 'was this not the best movie ever?'\n",
    "\n",
    "Will have very similar representations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ngrams to the Rescue\n",
    "#### Word Ngram \n",
    "![](https://i.stack.imgur.com/8ARA1.png)\n",
    "\n",
    "Character Ngram: For people who do not like to tokenize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Ngrams to the Rescue(2)\n",
    "** Problem with dimensionality ** \n",
    "Quad-grams Qvint Grams etc. Generates exponential number of features.\n",
    "\n",
    "*** Solution *** Pick only the ngrams using statistical analysis of the word co-occurences.\n",
    "Check out methods for doing so in the Natural Language Processing Toolkit package nltk: `nltk.collocations`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bag of Words features as input to Baseline models\n",
    "- Baseline models: Naive Bayes, Logistic Regresion, K-nearest Neighbor and Support Vector Machines\n",
    "\n",
    "The seem to work pretty well for many tasks:\n",
    "Wang and Manning (2015): *\"Baselines and Bigrams: Simple, Good Sentiment and Topic Classification\"* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Baseline Models(2) - Understanding the task at hand.\n",
    "\n",
    "* Always run simple BoW model first. Check if you need a more complex model, and evaluate models based on gains in relation to a bag-of-word model.\n",
    "\n",
    "\n",
    "* How hard is the learning task?\n",
    "    * How well \"linear\" and BoW models do. \n",
    "        * Learning what can be learned from atomized words alone tells you alot about the task at hand.\n",
    "    * Look at examples of where they fail.\n",
    "    * How much data you need. Learning curve.\n",
    "    \n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# From text to vector(2) - Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# simple example\n",
    "string = 'I really love bacon :)'\n",
    "string.split()\n",
    "#import re\n",
    "#token_re = re.compile('\\w+')\n",
    "#token_re.findall(string)\n",
    "\n",
    "#token_re = re.compile('\\w+|:\\)')\n",
    "#token_re.findall(string)\n",
    "import nltk\n",
    "# nltk.word_tokenize(string) # problem with emojies\n",
    "import nltk.sentiment \n",
    "# nltk.sentiment.vader.SentiText(string).words_and_emoticons # removal of stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Lets see how it does on a real dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv') # load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nltk 5282\n",
      "regex 5075\n",
      "nltk_sentiment 5489\n",
      ".split 6965\n"
     ]
    }
   ],
   "source": [
    "tokenizers = {'nltk':nltk.word_tokenize,'regex':lambda x: token_re.findall(x)\n",
    "              ,'nltk_sentiment':lambda x :nltk.sentiment.vader.SentiText(x).words_and_emoticons,'.split':\n",
    "             lambda x: x.split()}\n",
    "from collections import Counter\n",
    "string = ' '.join(df.reviewBody.values[0:1000])\n",
    "for name,tokenize in tokenizers.items():\n",
    "    print(name,len(Counter(tokenize(string))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# From Tokens to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bag of Words - Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CounterVectorizer() # handles preprocessing and tokenization by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Quick digression - Problem of Large feature spaces.\n",
    "* Stopword removal ['i','you','and','or'].'nltk.stopwords\n",
    "* Stemming and Lemmatizing: \n",
    "    * Reducing a word to its basic form. Walking --> walk. Are --> be. nltk.stem,nltk.lemmatize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# TFIDF - Term Frequency Inverse Document Frequency\n",
    "A clever weighing scheme (especially used in unsupervised learning)\n",
    "\n",
    "This is essentially removes the problem of irrelevant words, that are shared by all documents. And puts weight on the defining word of a document.\n",
    "\n",
    "It does this usign the following formula:\n",
    "\n",
    "$ IDF = \\log\\frac{N}{n_t} $\n",
    "\n",
    "$ TF = \\frac{c_{t_i,d}}{d_c} $\n",
    "\n",
    "where \n",
    "\n",
    "$N$ is the number of documents.\n",
    "\n",
    "$n_t$ is the number of documents with the token present\n",
    "\n",
    "$c_{t,d}$ is the is the number of times a token t is present in d\n",
    "\n",
    "$c_d$ is the number of tokens in document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<50x677 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1315 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer() # handles preprocessing and tokenization by default\n",
    "X = vectorizer.fit_transform(df.reviewBody.values[0:50])\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### And now it is your turn. Go to Exercise Set 15 and use this as input to a classifier."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
