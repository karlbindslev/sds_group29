{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 15: Text as Data 1\n",
    "\n",
    "*Morning, August 22, 2018*\n",
    "\n",
    "In this Exercise Set you will implement a sklearn classifier to do Sentiment Analysis using the labeled review data that you collected in exercise set 8. You will also practice your basic python skills while implementing the tf-idf weighing scheme. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Section 15.1: Writing your own TFIDF vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will practice your python skills while implementing the [Term Frequency - Inverse Document Frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) scheme.\n",
    "> **Ex. 15.1.0:** First we load the data: using the `pd.read_csv` function. link to the data is here: 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv \n",
    "\n",
    "> Next we define a variable `tokenized` to be transformed using our TF-IDF vectorizer, by tokenizing a the text column (reviewBody) in the dataframe using the `nltk.word_tokenize` function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are to define a our tfidf transformation of the tokenized texts. \n",
    "Remember that:\n",
    "$ IDF = \\log\\frac{N}{n_t} $\n",
    "\n",
    "$ TF = \\frac{c_{t_i,d}}{d_c} $\n",
    "\n",
    "where \n",
    "\n",
    "$N$ is the number of documents.\n",
    "\n",
    "$n_t$ is the number of documents with the token present\n",
    "\n",
    "$c_{t,d}$ is the is the number of times a token t is present in d\n",
    "\n",
    "$c_d$ is the number of tokens in document\n",
    "\n",
    "We need to do the following steps:\n",
    "1. For each word count the number of Document it is present in.\n",
    "2. Transform this document count into inverse document frequency. \n",
    "3. Calculate the term frequncy in each document.\n",
    "4. Finally we weight the term frequency in each document with the inverse document frequency of each term.\n",
    "5. We return this as a sparse vector. \n",
    "\n",
    "> **Ex. 15.1.1:** \n",
    "Import the Counter object from the builtin package collections (Hint1). This is essentially a dictionary designed for keeping counts, same syntax, but extra functionality. We don't have to initialize each key. We can write: \n",
    "\n",
    "```python\n",
    "c = Counter()\n",
    "# then we can do this\n",
    "c['hej']+=1\n",
    "# without first defining c['hej'] = 0\n",
    "```\n",
    "\n",
    "\n",
    ">* Initialize a Counter object and assign it to the variable `dc` (document count).\n",
    ">* Define a list named `text_counts`. In this container we will store each document after we have converted it to counts of tokens.\n",
    ">* Run through all tokenized texts and\n",
    "    * initialize a Counter object with the tokenized text as input, assign this object to a variable `c_t`. >This will now contain a count of each token in the document. Append `c_t` to our list `text_counts`.\n",
    "    * run though each key in the `c_t` and increment the document count variable `dc` by one. (Hint2)\n",
    "\n",
    "(hint1: from ... import ...)\n",
    "\n",
    "(hint2: dc[token]+=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer goes here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.1.2:** \n",
    "Now we define the the inverse document frequency variable `idf` as a dictionary with the tokens as keys and idf weights as values. We do this by running through both the token and the value (document count) in the `dc` variable and calculate the ratio between number documents and the token document counts. \n",
    "\n",
    ">Use the `np.log` function for the log transform.\n",
    "\n",
    ">We can iterate through this using the `.items()` syntax we know from the dictionary. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.1.3:** \n",
    "Now we weight the term frequency in each document with the idf value of each token. Here we used our `text_counts` variable that almost holds the frequency, we just need to divide by the number of tokens in the document. \n",
    "Define a list container: `tfidf_docs`. \n",
    "\n",
    "FIRST LOOP: For each counter in the text_count container:\n",
    "    * define the variable `doc_n` as sum of all values in the counter - `.values()` .\n",
    "    * define a dictionary named `tfidf`.\n",
    "    * SECOND LOOP: run through all tokens, and their counts by using the `.items()` method of the counter.\n",
    "        * define a value tf as the ratio between the count and the sum.\n",
    "        * now weight this value with the idf weight found by calling the idf variable with the token as key.\n",
    "        * assign this weighed term frequency to the tfidf[token].\n",
    "    * Once outside the second loop. Append the tfidf dictionary to the tfidf_docs list container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.1.extra:** \n",
    "Convert the dictionary to a sparse matrix.\n",
    "* Create a index for each token that you can look up using a dictionary. \n",
    "* define the shape of the matrix i.e. n_rows and n_cols, as a tuple containing the number of documents and number of tokens.\n",
    "* import scipy.sparse as sp. And initialize a sparse matrix you can build incrementally: sp.lil_matrix(). \n",
    "    * It takes the shape parameter. And a datatype `dtype` parameter, define the dtype as np.float32. \n",
    "* Iterate through the transformed documents from the `tfidf_docs` variable. Add the enumerate() function to keep of track of the row numbers. \n",
    "    * SECOND LOOP: iterate through all token, and tfidfscore. Get the index of the token and assign the score to the matrix using doc_idx and token_idx as selectors. i.e. mat[doc_idx,token_idx] = score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 15.1.extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Section 15.2: Supervised Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise I want you to train a classifier to do sentiment analysis of text. You will use the ratings as labels and the reviews as features. You will go through all steps, from preprocessing, feature engineering, cleaning and tokenization, to vectorization and training of the classifier. Then you will wrap it all in a function to make the code reusable. \n",
    "\n",
    "And finally you will analyze the performance of the resulting classifier.\n",
    "> **Ex. 15.2.0:** First we load the data: using the `pd.read_csv` function. link to the data is here: 'https://raw.githubusercontent.com/snorreralund/scraping_seminar/master/english_review_sample.csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 15.2.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering using regular expression.\n",
    "Because we are essentially creating a model that trains on a bag of words representation of the data, we are not going to think too much about the tokenization scheme. However we want to make sure that emoticons and emojiis are being included as they carry vital information for sentiment analysis. Here we use the regular expression to capture emoticons we wrote in Exercise set 9 or write a new one. \n",
    "\n",
    "Furthermore we want to continue exercise 9.2 of capturing references to the cost of a service. We want to embed domain knowledge into the prices, convert it to a categorical variable, from low to high price, instead of the model being presented with unique tokens ($\\$921$ $\\$10$ $\\$935$ ) that it cannot learn from. \n",
    "\n",
    "**Ex. 15.2.1:** \n",
    "Write a function to capture all digits before or after a dollar sign. But we only capture the digits. We then convert it to a float. And map this to a categorical value. We convert the digit using the following rules: \n",
    "* if below $\\$10$: return '\\__price0__'\n",
    "* elif between $\\$10$ and $\\$100$: return '\\_\\_price1\\__'\n",
    "* elif between $\\$100$ and $\\$500$: return '\\_\\_price2\\__'\n",
    "* else: return '\\__price3\\__'\n",
    "\n",
    "Instructions here:\n",
    ">* First write a function `price2category`that takes a float or integer value and outputs a price category (e.g. 100000 = '\\__price3__') according to the above rules.\n",
    "* Compile your currency regular expression and assign it to the variable `currency_re`.\n",
    "* Use the currency_re variable to find all (.findall) prices/currencies mentioned in a string. Assign this to a variable: `prices`\n",
    "* define a simple a regular expression that extract only the digits from a string you already know is a price.\n",
    "* Extract the digits from each price string in the `prices` variable, and assign them to a list named `digits`.\n",
    "* Now we need to distinquish between ,. used to indicate fraction of a dollar, or to help us read large numbers. Here we use the following patterns that looks behind and ahead and counts how many digits. two digits after indicates fraction of a dollar, and 3 digits indicate helper.: \n",
    "```python \n",
    "helper_pattern = '(?<=\\d)[.,](?=\\d{3})' # help pattern \n",
    "cent_pattern = '(?<=\\d)[,.](?=\\d{2})' # cent pattern\n",
    "```\n",
    "* Use the cent_pattern to substitute all ,. with '.'. I.e. by applying re.sub(cent_pattern,digit_string) to all digits strings in the `digits` variable. \n",
    "* Do same thing with the helper_pattern but now substitute with an empty string ''. \n",
    "* Convert the now ready digit strings to float. Using the builtin function `float()`.\n",
    "* Then convert all float values to pricecategories by applying the price2category function on all the values. \n",
    "* Lastly iterate through the original matches in the string, stored in the `prices` variable, and the resulting pricecategories using a for loop where you zip the two variables: zip(prices,price_categories). \n",
    "    * For each price, pricecategory pair you overwrite the original string, with a string that has replaced the price with the pricecategory.\n",
    "\n",
    "* Finally wrap it all in a function named `embed_price_categories` and remember to return the string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.2.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.2.2:** Normalization and tokenization\n",
    "In this exercise we define a function for normalizing tokens. It should apply our feature engineering of prices, make sure emoticons are tokenized, lower string to noncapital letters.  \n",
    "\n",
    ">* Define a function `normalize_tokens` taken a string as input. \n",
    "* First use the function for extracting prices and substituting for a price class.\n",
    "* Because the standard tokenization scheme does not have a rule for emoticons we extract all emoticons before tokenization. Do this by using the following precompiled regular expression from the `nltk` package:\n",
    "nltk.sentiment.util.EMOTICON_RE.findall() - compiled here means that you don't need to specify the pattern, since it is build in. You need to `import nltk.sentiment`.\n",
    "* If emoticons are found, iterate through the emoticons found and remove them from the string using the builtin string method: `.replace`.\n",
    "* Now write a list comprehension lowering all strings - i.e. capital to noncapital letters. Use the builtin string method: `.lower()`\n",
    "* finally add the emoticons found to the token list. And return all tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer 15.2.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.2.3** Now we are ready to convert our documents into Sparse Matrices to be used in training the classifier. But first we need to convert our ratings variable into a binary form and split the data into train and test. \n",
    "* apply function that return 0 if rating is 3 or below and 1 if rating is above. \n",
    "* Next we split our data into test and train, by indexing the first 7500 for traning and last 2500 for testing. \n",
    "* We use the sklearn version of the tfidf vectorizer. First import it (if you don't know how to, ask google). \n",
    "* Initialize the vectorizer with the arguments, preprocessor = None, tokenizer=normalize_tokens.\n",
    "* Apply the `.fit` function to the training data only (to make sure no leakage from train to test will happen).\n",
    "* apply the `.transform` function to both the training and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.2.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 15.2.4:** Training the model. \n",
    "Here we apply a logistic regression model with regularization to predict whether the rating is positive (above 3) or negative. \n",
    "* First we import the classifier: from sklearn.linear_model import LogisticRegression \n",
    "* Next we initialize it with regularization parameter C=10\n",
    "* Then we use the .fit method.\n",
    "* And finally we measure the performance: accuracy, precision, recall, f1 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.2.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Ex. 15.2.5:**\n",
    "Now run the classifier again + evaluation, but this time we do a multiclass prediction. This means changing the `y` variable to be the ratings.\n",
    "\n",
    "* When doing the evaluation, we also want to see the confusion matrix to inspect the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer 15.2.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bias and Fairness\n",
    "If we want to use our classifier as a measurement tool, for say measuring public sentiment. We need to understand the bias our classifier has so we can potentially correct it. \n",
    "In this you should Calculate performances on subpopulations of the data.\n",
    "\n",
    "* We should look at how it is skewed towards one class or the other.\n",
    "* We should look at if it does better under certain product categories.\n",
    "* We should look at whether it does better when male or female authors. (by inferring gender matching the surname to data from the following register: (Female names: https://ast.dk/_namesdb/export/names?format=xls&gendermask=1, male names: https://ast.dk/_namesdb/export/names?format=xls&gendermask=2, unisex names: https://ast.dk/_namesdb/export/names?format=xls&gendermask=3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[Answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra\n",
    "Design a regular expression that locates references to time (days, months, minutes hours) and do a similar categorization using domain knowledge, of what long time is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
