{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-23T15:30:03.634114Z",
     "start_time": "2017-08-23T15:30:03.629294Z"
    }
   },
   "source": [
    "# Exercise Set 14: Classification Trees\n",
    "\n",
    "*Afternoon, August 21, 2018*\n",
    "\n",
    "In this Exercise Set 14 we will explore some of the most used tree-based classifiers, beginning with simple decision tree classifiers. \n",
    "\n",
    "\n",
    "> **Ex. 14.1.1:** In this exercise set we will work with the titanic dataset. Run the code below to get a variable **data**. Notice the last line of the code - what does it do?\n",
    ">\n",
    ">```python\n",
    "> rawdata = sns.load_dataset('titanic').sample(frac=1, random_state = 3)\n",
    "> ```\n",
    "># Feature creation\n",
    "> ```python\n",
    ">rawdata['male'] = (rawdata['sex'] == 'male').astype(int)\n",
    ">rawdata['alone'] = rawdata['alone'].astype(int)\n",
    ">rawdata['adult_male'] = rawdata['adult_male'].astype(int)\n",
    "> ```\n",
    "> The following piece of code (in multiple lines) generates dummies for all of the categorical variables. \n",
    "> ```\n",
    "data = pd.get_dummies(rawdata, \n",
    "                      columns = ['class', 'sibsp', 'parch', 'deck'], \n",
    "                      drop_first=True)\\\n",
    "         .drop(['pclass', 'sex', 'embarked', 'who', 'embark_town', 'alive'], axis = 1)\n",
    ">```\n",
    "> Read about decision-tree classification [here](http://scikit-learn.org/stable/modules/tree.html#classification). Try to explain why decision tree classification can _only_ produce splits orthogonal to one of the axes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.2:** Use `data` to create a numpy array **X** and an array **y**, y should contain the variable called 'survived', and X should contain all the other variables. Remove all rows containing missing values beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.3**: Train a decision tree classifier on **all** of the titanic data.\n",
    ">\n",
    ">* If your feature matrix is `X` and your target array is `y` you can do this by instantiating a model like:\n",
    ">\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        model = DecisionTreeClassifier()\n",
    "        model.fit(X, y)  # <--- This is the training/fitting/learning step\n",
    ">       \n",
    ">Write four functions that counts the number of ..\n",
    " - true positives where we call the function `TP`;\n",
    " - true negatives where we call the function `TN`;\n",
    " - false positives where we call the function `FP`;\n",
    " - false negatives where we call the function `FN`.\n",
    "\n",
    "> All of these functions should take three arguments, the actual y column, the actual X column and a fittedModel object (e.g. `fittedModel = DecisionTreeClassifier.fit(X,y)`)\n",
    ">\n",
    ">> _Hint 1:_ use the function `np.where` to compare y and the predicted values. For example `y + prediction == 2` is true only for the True Positives.\n",
    ">\n",
    ">> _Hint 2:_ You can check if your result is correct by summing all four functions and checking that the result equals to the number of observations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.4:** Combine the four functions you defined above to write your own accuracy function, which calculates\n",
    "$$\n",
    "ACC = \\frac{TP + TN}{TP+ TN + FP + FN}\n",
    "$$\n",
    ">\n",
    "> Test the accuracy of your model using the `A` function. Report the accuracy of your model on the same data that you trained the model on.\n",
    ">\n",
    ">> _Note:_ The reason we want to split the calculation of accuracy into these four components, is that we can then easily calculate other scores, such as the _precision, recall and f1_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.5**: So far our model has been achieving an unbelievably high accuracy of 100% correct classification.\n",
    ">\n",
    ">1. Why did you get such a high accuracy in the previous exercises?\n",
    ">2. Split your data into a test and training set of equal size. Train the model only on the training set and report its accuracy on both the training and test sets.\n",
    ">> Hint: `from sklearn.model_selection import train_test_split` is an easy way to split your data.\n",
    ">3. Comment on the difference you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.6:** Read about the random forest classifier in [the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) - how does the RF learner relate to decision trees? Train a random forest classifier, and print its train-accuracy and test-accuracy. How does it compare to the decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.7:** Random forest have a number of hyperparameters that we can use. Often we just use the default settings as they are shown to be pretty good. Sometimes we want to make sure we grow the right number of trees to grow in the forest. Try varying `n_estimators` and plot the resulting curve. \n",
    "\n",
    ">> *Hint:* try the `validation_curve` which we learned about in Ex. 13.1.4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.8:** Often we want to see the learning curve which shows model scores for test and train data with increasing sample sizes. Try making this plot. Be sure to use the n_estimators you have deemed useful.\n",
    "\n",
    "> A balanced model should show a declining train and an increasing test score when adding more observations. Is this the case? \n",
    "\n",
    ">> *Hint:* try the `validation_curve` which we learned about in Ex. 13.1.4.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.8 BONUS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 14.1.9 (BONUS):** We want to reduce the overfitting in our random forest. Read through the random forest and check which hyperparameters may help mitigating overfitting. You can use validation curves to search these. Try to making a new learning curve.\n",
    "\n",
    ">> *Hint 1:* Can the number of features used in bootstrapping also be considered a hyperparameter? \n",
    "\n",
    ">> *Hint 2:* Can the depth of trees be considered a hyperparameter? \n",
    "\n",
    ">> *Hint 3:* sklearn has the built-in `learning_curve` in model_selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to Ex. 14.1.9 BONUS]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
